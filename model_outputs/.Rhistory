# Q4.3
finnish <- read.table(url("http://socsci.uci.edu/~cjmayer/LSCI_202A/finnish.txt"))
uyghur <- read.table(url("http://socsci.uci.edu/~cjmayer/LSCI_202A/uyghur.txt"))
finnish$V1 %>%
get_cooccurrence_proportion(c('y', 'Ã¶', 'Ã¤'), c('u', 'o', 'a'))
get_cooccurrence_tokens(finnish, c('y', 'Ã¶', 'Ã¤'), c('u', 'o', 'a'))
finnish
typeof(finnish)
get_cooccurrence_tokens <- function(corpus, set1, set2){
re1 <- str_glue("[{str_c(set1, collapse='')}]")
re2 <- str_glue("[{str_c(set2, collapse='')}]")
corpus %>%
str_subset(re1) %>%
str_subset(re2)
}
get_cooccurence_tokens(finnish, c('y', 'ö', 'ä'), c('u', 'o', 'a'))
get_cooccurrence_tokens <- function(corpus, set1, set2){
re1 <- str_glue("[{str_c(set1, collapse='')}]")
re2 <- str_glue("[{str_c(set2, collapse='')}]")
corpus %>%
str_subset(re1) %>%
str_subset(re2)
}
# Q4.2
get_cooccurrence_proportion <- function(corpus, set1, set2){
length(get_cooccurrence_tokens(corpus, set1, set2)) / length(corpus)
}
# Q4.3
finnish <- read.table(url("http://socsci.uci.edu/~cjmayer/LSCI_202A/finnish.txt"))
uyghur <- read.table(url("http://socsci.uci.edu/~cjmayer/LSCI_202A/uyghur.txt"))
finnish$V1 %>%
get_cooccurrence_proportion(c('y', 'ö', 'ä'), c('u', 'o', 'a'))
uyghur$V1 %>%
get_cooccurrence_proportion(c('ü', 'ö', 'e'), c('u', 'o', 'a'))
finnish <- read.table(url("http://socsci.uci.edu/~cjmayer/LSCI_202A/finnish.txt"), encoding='UTF-8')
uyghur <- read.table(url("http://socsci.uci.edu/~cjmayer/LSCI_202A/uyghur.txt"), encoding='UTF-8')
finnish$V1 %>%
get_cooccurrence_proportion(c('y', 'ö', 'ä'), c('u', 'o', 'a'))
uyghur$V1 %>%
get_cooccurrence_proportion(c('ü', 'ö', 'e'), c('u', 'o', 'a'))
get_cooccurence_tokens(finnish$V1, c('y', 'ö', 'ä'), c('u', 'o', 'a'))
get_cooccurrence_tokens(finnish$V1, c('y', 'ö', 'ä'), c('u', 'o', 'a'))
bar <- get_cooccurrence_tokens(finnish$V1, c('y', 'ö', 'ä'), c('u', 'o', 'a'))
length(bar)
bar <- get_cooccurrence_tokens(uyghur$V1, c('ü', 'ö', 'e'), c('u', 'o', 'a'))
length(bar)
?get_cooccurrence_tokens
get_cooccurrence_tokens
# Returns disharmonic roots
get_cooccurence_tokens <-function(corpus,set1,set2){
# create regex patterns
set1_str <- str_c(set1,collapse='')
set1_pattern <- str_glue('[{set1_str}]')
set2_str <- str_c(set2,collapse='')
set2_pattern <- str_glue('[{set2_str}]')
# load corpus
corpus_data <- read_csv(corpus,col_names=c('words'))
# extract disharmonic roots into a vector
disharmonic_roots <- corpus_data %>%
filter(str_detect(words,set1_pattern) & str_detect(words,set2_pattern)) %>%
pull(words)
return(disharmonic_roots)
}
# Returns proportion of disharmonic roots in the corpus
get_cooccurence_proportion <- function(corpus,set1,set2){
# load corpus
corpus_data <- read_csv(corpus,col_names=c('words'))
C = dim(corpus_data)[1]
# get disharmonic roots
disharmonic_roots <- get_cooccurence_tokens(corpus,set1,set2)
d = length(disharmonic_roots)
# compute proportion
proportion_disharmonic <- d/C
return(proportion_disharmonic)
}
# Finnish
Finnish_corpus <- 'http://socsci.uci.edu/~cjmayer/LSCI_202A/finnish.txt'
# Front vowels in Finnish
set1 <- c('y','ö','ä')
# Back vowels in Finnish
set2 <- c('u','o','a')
Finnish_disharmonic_roots <- get_cooccurence_tokens(Finnish_corpus,set1,set2)
Finnish_disharmonic_proportion <- get_cooccurence_proportion(Finnish_corpus,set1,set2)
print(Finnish_disharmonic_proportion)
# Uyghur
Uyghur_corpus <- 'http://socsci.uci.edu/~cjmayer/LSCI_202A/uyghur.txt'
# Front vowels in Uyghur
set1 <- c('ü','ö','e')
# Back vowels in Uyghur
set2 <- c('u','o','a')
Uyghur_disharmonic_roots <- get_cooccurence_tokens(Uyghur_corpus,set1,set2)
Uyghur_disharmonic_proportion <- get_cooccurence_proportion(Uyghur_corpus,set1,set2)
print(Uyghur_disharmonic_proportion)
??knitr::opts_chunk
cars
geom_scatter
library(tidyverse)
geom_pocars
cars
install.packages("blogdown")
blogdown::install_hugo()
blogdown::install_hugo()
blogdown::install_hugo()
blogdown::install_hugo()
library(tidyverse)
data <- read_csv("C:/Users/conno/Dropbox/ling/megha_grant/distributional_learning/code/output.csv")
data <- data %>%
mutate(CosSimQuart = cut(data$CosSim, 4, labels=c('1', '2', '3', '4')),
PCACosSimQuart = cut(data$PCACosSim, 4, labels=c('1', '2', '3', '4')),
FeatCosSimQuart = cut(data$FeatCosSim, 4, labels=c('1', '2', '3', '4'))
)
low_dist_low_phon <- data %>%
filter(PCACosSimQuart == 1 & FeatCosSimQuart == 1) %>%
arrange(PCACosSim)
low_dist_high_phon <- data %>%
filter(PCACosSimQuart %in% c(1,2) & FeatCosSimQuart %in% c(3,4)) %>%
arrange(Class)
high_dist_low_phon <- data %>%
filter(PCACosSimQuart %in% c(3) & FeatCosSimQuart %in% c(1)) %>%
arrange(PCACosSim)
high_dist_high_phon <- data %>%
filter(PCACosSimQuart %in% c(4) & FeatCosSimQuart %in% c(4)) %>%
arrange(PCACosSim)
high_dist_low_phon
order
high_dist_low_phon
data <- read_csv("C:/Users/conno/Dropbox/ling/megha_grant/ngram calculator/output.csv")
library(tidyverse)
data <- read_csv("C:/Users/conno/Dropbox/ling/megha_grant/ngram calculator/output.csv")
cor(uni_prob, pos_uni_freq)
cor(data$uni_prob, data$pos_uni_freq)
cor(data$bi_prob, data$pos_bi_freq)
library(tidyverse)
data <- read_csv("C:/Users/conno/Dropbox/ling/megha_grant/ngram calculator/output.csv")
data
cor(data[data$word_len == 3,]$bi_prob, data[data$word_len == 3,]$pos_bi_freq)
cor(data[data$word_len == 3,]$uni_prob, data[data$word_len == 3,]$pos_uni_freq)
cor(data[data$word_len == 2,]$uni_prob, data[data$word_len == 2,]$pos_uni_freq)
arrange
arrange(data, vl_score)
arrange(data, -vl_score)
arrange(data[data$word_len == 3,], -vl_score)
library(tidyverse)
data <- read_csv("C:/Users/conno/Dropbox/ling/megha_grant/ngram calculator/output.csv")
library(tidyverse)
data <- read_csv("C:/Users/conno/Dropbox/ling/megha_grant/ngram calculator/output.csv")
data
data %>% group_by(word_len) %>% mutate()
data %>% group_by(word_len) %>% mutate(foo = cor(bi_prob, pos_bi_freq))
data %>% group_by(word_len) %>% mutate(foo = cor(bi_prob, pos_bi_freq))
blah <- data %>% group_by(word_len) %>% mutate(foo = cor(bi_prob, pos_bi_freq))
blah
blah <- data %>% group_by(word_len) %>% summarize(foo = cor(bi_prob, pos_bi_freq))
blah
head(data)
correlations <- data %>%
group_by(word_len) %>%
summarize(
unigram_uniphone_cor = cor(uni_prob, pos_uni_freq)
bigram_biphone_cor = cor(bi_prob, pos_bi_freq))
correlations <- data %>%
group_by(word_len) %>%
summarize(
unigram_uniphone_cor = cor(uni_prob, pos_uni_freq),
bigram_biphone_cor = cor(bi_prob, pos_bi_freq))
correlations
write_csv(correlations, 'correlations.csv')
mean(correlations$unigram_uniphone_cor)
mean(correlations$bigram_biphone_cor)
mean(correlations$bigram_biphone_cor, na.rm=TRUE)
write_csv(correlations, 'C:/Users/conno/Dropbox/ling/megha_grant/ngram calculator/correlations.csv')
library(tidyverse)
data <- read_csv("C:/Users/conno/Dropbox/ling/megha_grant/ngram calculator/output_stimuli.csv")
correlations <- data %>%
group_by(word_len) %>%
summarize(
unigram_uniphone_cor = cor(uni_prob, pos_uni_freq),
bigram_biphone_cor = cor(bi_prob, pos_bi_freq)
)
write_csv(correlations, 'C:/Users/conno/Dropbox/ling/megha_grant/ngram calculator/correlations_stimuli.csv')
correlations
data
unique(data$word)
unique(data$word_length)
unique(data$word_len)
correlations
data
data[data$word_len == 2,]
data[data$word_len == 4,]
correlations <- data %>%
summarize(
unigram_uniphone_cor = cor(uni_prob, pos_uni_freq),
bigram_biphone_cor = cor(bi_prob, pos_bi_freq)
)
correlations
correlations <- data %>%
filter(word_len == 3) %>%
summarize(
unigram_uniphone_cor = cor(uni_prob, pos_uni_freq),
bigram_biphone_cor = cor(bi_prob, pos_bi_freq)
)
data[data$word_len == 3,]
data[data$word_len == 3 & is.na(bi_prob),]
data[data$word_len == 3 & is.na(data$bi_prob),]
data[is.na(data$bi_prob)]
data[is.na(data$bi_prob),]
data[is.na(data$uni_prob),]
data[is.na(data$pos_uni_freq),]
data[is.na(data$pos_bi_freq),]
library(tidyverse)
data <- read_csv("C:/Users/conno/Dropbox/ling/megha_grant/ngram calculator/output_stimuli.csv")
correlations <- data %>%
filter(word_len == 3) %>%
summarize(
unigram_uniphone_cor = cor(uni_prob, pos_uni_freq, na.rm=TRUE),
bigram_biphone_cor = cor(bi_prob, pos_bi_freq, na.rm=TRUE)
)
data
data[data$bi_prob == '-inf']
data[data$bi_prob == '-inf',]
data[data$bi_prob == -inf,]
data[data$word == 'Y AW T',]
data[data$bi_prob == -Inf,]
library(tidyverse)
data <- read_csv("C:/Users/conno/Dropbox/ling/megha_grant/ngram calculator/output_stimuli.csv")
correlations <- data %>%
filter(word_len == 3) %>%
summarize(
unigram_uniphone_cor = cor(uni_prob, pos_uni_freq),
bigram_biphone_cor = cor(bi_prob, pos_bi_freq)
)
write_csv(correlations, 'C:/Users/conno/Dropbox/ling/megha_grant/ngram calculator/correlations_stimuli.csv')
correlations
correlations <- data %>%
summarize(
unigram_uniphone_cor = cor(uni_prob, pos_uni_freq),
bigram_biphone_cor = cor(bi_prob, pos_bi_freq)
)
correlations
correlations <- data %>%
group_by(word_len) %>%
summarize(
unigram_uniphone_cor = cor(uni_prob, pos_uni_freq),
bigram_biphone_cor = cor(bi_prob, pos_bi_freq)
)
correlations
library(tidyverse)
data <- read_csv("C:/Users/conno/Dropbox/ling/megha_grant/ngram calculator/output_stimuli.csv")
correlations <- data %>%
filter(word_len == 3) %>%
summarize(
unigram_uniphone_cor = cor(uni_prob, pos_uni_freq),
bigram_biphone_cor = cor(bi_prob, pos_bi_freq)
)
write_csv(correlations, 'C:/Users/conno/Dropbox/ling/megha_grant/ngram calculator/correlations_stimuli.csv')
correlations
t.test
t.test(c(1,2,3), c(6,2,8))
?t.test
filename <- 'C:/Users/conno/Dropbox/ling/vowel_learning_project/Code/vowel_learners/outputs/spanish_ads_cds/ads_results_f1_f2.csv'
df <- read_csv(filename)
library(tidyverse)
df <- read_csv(filename)
df
df <- read_csv(filename)
df
ggplot(df, aes(x=f1, y=f2, group=learned_cats)) +
geom_point()
ggplot(df, aes(x=f1, y=f2, color=learned_cats)) +
geom_point()
ggplot(df, aes(x=f1, y=f2, color=as.factor(learned_cats))) +
geom_point()
filename <- 'C:/Users/conno/Dropbox/ling/vowel_learning_project/Code/vowel_learners/outputs/spanish_ads_cds/cds_results_f1_f2.csv'
df <- read_csv(filename)
ggplot(df, aes(x=f1, y=f2, color=as.factor(learned_cats))) +
geom_point()
ggplot(df, aes(x=f1, y=f2, color=as.factor(learned_cats))) +
geom_point(size=10)
ggplot(df, aes(x=f1, y=f2, color=as.factor(learned_cats))) +
geom_point(size=5)
ggplot(df, aes(x=f1, y=f2, color=as.factor(learned_cats))) +
geom_point(size=6)
ads <- read_csv(filename)
ggplot(ads, aes(x=f1, y=f2, color=as.factor(learned_cats))) +
geom_point(size=5)
filename <- 'C:/Users/conno/Dropbox/ling/vowel_learning_project/Code/vowel_learners/outputs/spanish_ads_cds/ads_results_f1_f2.csv'
ads <- read_csv(filename)
ggplot(ads, aes(x=f1, y=f2, color=as.factor(learned_cats))) +
geom_point(size=5)
ads_filename <- 'C:/Users/conno/Dropbox/ling/vowel_learning_project/Code/vowel_learners/outputs/spanish_ads_cds/ads_results_f1_f2.csv'
ads <- read_csv(ads_filename)
ggplot(ads, aes(x=f1, y=f2, color=as.factor(learned_cats))) +
geom_point(size=5)
cds_filename <- 'C:/Users/conno/Dropbox/ling/vowel_learning_project/Code/vowel_learners/outputs/spanish_ads_cds/cds_results_f1_f2.csv'
cds <- read_csv(cds_filename)
ggplot(cds, aes(x=f1, y=f2, color=as.factor(learned_cats))) +
geom_point(size=5)
ggplot(ads, aes(x=f2, y=f1, color=as.factor(learned_cats))) +
geom_point(size=5)
ggplot(cds, aes(x=f2, y=f1, color=as.factor(learned_cats))) +
geom_point(size=5)
ggplot(ads, aes(x=f2, y=f1, color=as.factor(learned_cats))) +
geom_point(size=5)
ggplot(cds, aes(x=f2, y=f1, color=as.factor(learned_cats))) +
geom_point(size=5) +
scale_x_reverse() +
scale_y_reverse()
ggplot(ads, aes(x=f2, y=f1, color=as.factor(learned_cats))) +
geom_point(size=5) +
scale_x_reverse() +
scale_y_reverse()
ggplot(ads, aes(x=f2, y=f1, color=as.factor(learned_cats))) +
geom_point(size=3) +
scale_x_reverse() +
scale_y_reverse()
ggplot(cds, aes(x=f2, y=f1, color=as.factor(learned_cats))) +
geom_point(size=3) +
scale_x_reverse() +
scale_y_reverse()
file <- 'C:/Users/conno/Dropbox/ling/vowel_learning_project/Code/vowel_learners/corpus_data/sp_ids_rev.csv'
foo <- read_csv(file)
source("C:/Users/conno/Dropbox/ling/vowel_learning_project/Code/vowel_learners/R/plot_data.R", echo=TRUE)
foo
ggplot(data=foo) +
geom_point(aes(x=f1, y=f2, color=vowel)) +
ggtitle("Spanish")
}
analyze_results <- function(results_file, mu_file, covs_file, ll_file) {
# Look at true vs. learned category assignments
results <- read_csv(results_file)
ggplot(ads, aes(x=f2, y=f1, color=as.factor(vowel))) +
geom_point(size=3) +
scale_x_reverse() +
scale_y_reverse() +
labs(color="Vowel")
ggsave("../figures/spanish_ads_true_cats.png")
ggplot(ads, aes(x=f2, y=f1, color=as.factor(learned_cat))) +
geom_point(size=3, alpha=0.5) +
scale_x_reverse() +
scale_y_reverse() +
labs(color="Learned category")
ggsave("../figures/spanish_ads_f1_f2_learned_cats.png")
# Look at learned distributions
mus <- read_csv(mu_file)
covs <- read_csv(cov_file)
plot_distributions(mus, covs, results)
ggsave('../figures/spanish_ads_f1_f2_learned_cat_distributions.png')
# Look at LL
ll <- read_csv(ll_file)
ggplot(ll, aes(x=iteration, y=log_likelihood)) +
geom_line()
ggave('../figures/')
}
ads_file <- 'spanish_ads_f1_f2.csv'
tools::file_ext(ads_file)
FIGURES_DIR = 'C:/Users/conno/Dropbox/ling/vowel_learning_project/Code/vowel_learners/figures/'
FIGURES_DIR + name
results_file <- ads_file
results_file
name = strsplit(results_file, "\\.")[[1]]
name
name[[1]]
name[[1]]name
name
strsplit(results_file, "\\.")
strsplit(results_file, "\\.")[[[1]]]
strsplit(results_file, "\\.")[[1]
]
strsplit(results_file, "\\.")[[1]][1]
str_c
library(tidyverse)
str_c
str_c(name, '_true_cats.png')
name = strsplit(results_file, "\\.")[[1]][1]
str_c(name, '_true_cats.png')
analyze_results <- function(results_file, mu_file, covs_file, ll_file) {
name = strsplit(results_file, "\\.")[[1]][1]
# Look at true vs. learned category assignments
results <- read_csv(results_file)
ggplot(ads, aes(x=f2, y=f1, color=as.factor(vowel))) +
geom_point(size=3) +
scale_x_reverse() +
scale_y_reverse() +
labs(color="Vowel")
ggsave(file.path(FIGURES_DIR, str_c(name, '_true_cats.png')))
ggplot(ads, aes(x=f2, y=f1, color=as.factor(learned_cat))) +
geom_point(size=3, alpha=0.5) +
scale_x_reverse() +
scale_y_reverse() +
labs(color="Learned category")
ggsave(file.path(FIGURES_DIR, str_c(name, '_learned_cats.png')))
# Look at learned distributions
mus <- read_csv(mu_file)
covs <- read_csv(cov_file)
plot_distributions(mus, covs, results)
ggsave(file.path(FIGURES_DIR, str_c(name, '_learned_cat_distributions.png')))
# Look at LL
ll <- read_csv(ll_file)
ggplot(ll, aes(x=iteration, y=log_likelihood)) +
geom_line()
ggsave(file.path(FIGURES_DIR, str_c(name, 'll.png')))
}
analyze_results(ads_file, ads_mu_file, ads_cov_file, ll_file)
# Look at ADS
setwd('C:/Users/conno/Dropbox/ling/vowel_learning_project/Code/vowel_learners/model_outputs')
analyze_results(ads_file, ads_mu_file, ads_cov_file, ll_file)
analyze_results <- function(results_file, mu_file, covs_file, ll_file) {
name = strsplit(results_file, "\\.")[[1]][1]
# Look at true vs. learned category assignments
results <- read_csv(results_file)
ggplot(results, aes(x=f2, y=f1, color=as.factor(vowel))) +
geom_point(size=3) +
scale_x_reverse() +
scale_y_reverse() +
labs(color="Vowel")
ggsave(file.path(FIGURES_DIR, str_c(name, '_true_cats.png')))
ggplot(results, aes(x=f2, y=f1, color=as.factor(learned_cat))) +
geom_point(size=3, alpha=0.5) +
scale_x_reverse() +
scale_y_reverse() +
labs(color="Learned category")
ggsave(file.path(FIGURES_DIR, str_c(name, '_learned_cats.png')))
# Look at learned distributions
mus <- read_csv(mu_file)
covs <- read_csv(cov_file)
plot_distributions(mus, covs, results)
ggsave(file.path(FIGURES_DIR, str_c(name, '_learned_cat_distributions.png')))
# Look at LL
ll <- read_csv(ll_file)
ggplot(ll, aes(x=iteration, y=log_likelihood)) +
geom_line()
ggsave(file.path(FIGURES_DIR, str_c(name, 'll.png')))
}
analyze_results(ads_file, ads_mu_file, ads_cov_file, ll_file)
ads_file <- 'spanish_ads_f1_f2.csv'
ads_mu_file <- 'spanish_ads_mus_f1_f2.csv'
ads_cov_file <- 'spanish_ads_covs_f1_f2.csv'
ll_file <- 'spanish_ads_ll_f1_f2.csv'
analyze_results(ads_file, ads_mu_file, ads_cov_file, ll_file)
analyze_results <- function(results_file, mu_file, cov_file, ll_file) {
name = strsplit(results_file, "\\.")[[1]][1]
# Look at true vs. learned category assignments
results <- read_csv(results_file)
ggplot(results, aes(x=f2, y=f1, color=as.factor(vowel))) +
geom_point(size=3) +
scale_x_reverse() +
scale_y_reverse() +
labs(color="Vowel")
ggsave(file.path(FIGURES_DIR, str_c(name, '_true_cats.png')))
ggplot(results, aes(x=f2, y=f1, color=as.factor(learned_cat))) +
geom_point(size=3, alpha=0.5) +
scale_x_reverse() +
scale_y_reverse() +
labs(color="Learned category")
ggsave(file.path(FIGURES_DIR, str_c(name, '_learned_cats.png')))
# Look at learned distributions
mus <- read_csv(mu_file)
covs <- read_csv(cov_file)
plot_distributions(mus, covs, results)
ggsave(file.path(FIGURES_DIR, str_c(name, '_learned_cat_distributions.png')))
# Look at LL
ll <- read_csv(ll_file)
ggplot(ll, aes(x=iteration, y=log_likelihood)) +
geom_line()
ggsave(file.path(FIGURES_DIR, str_c(name, 'll.png')))
}
analyze_results(ads_file, ads_mu_file, ads_cov_file, ll_file)
plot_distributions <- function(mu, cov, results) {
ggplot(data=mu, aes(x=f2, y=f1, label=vowel)) +
scale_x_reverse() +
scale_y_reverse() +
geom_text()
graph <- ggplot()
cov <- cov %>%
select('vowel', 'f1-f1', 'f1-f2', 'f2-f1', 'f2-f2')
el = c()
for (i in 1:nrow(mu)) {
cov_vals <- matrix(as.numeric(cov[i, 2:ncol(cov)]), ncol=2) / nrow(results %>% filter(learned_cat == i - 1))
v_el <- ellipse(cov_vals, centre=matrix(mu[i,] %>% select('f1', 'f2') %>% as.numeric))
v_el <- as_tibble(v_el)
v_el$vowel <- mu[i,]$vowel
el <- rbind(el, v_el)
}
graph <- graph + geom_point(data=el, aes(x=y, y=x, color=as.factor(vowel)), size=4)
graph +
geom_label(data=mu, aes(x=f2, y=f1, label=vowel, color=as.factor(vowel)), size=4) +
scale_x_reverse() +
scale_y_reverse() +
xlab("F2") +
ylab("F1") +
theme(legend.position="none")
}
library(ellipse)
analyze_results(ads_file, ads_mu_file, ads_cov_file, ll_file)
# Look at CDS
cds_file <- 'spanish_cds_f1_f2.csv'
cds_mu_file <- 'spanish_cds_mus_f1_f2.csv'
cds_cov_file <- 'spanish_cds_covs_f1_f2.csv'
cds_ll_file <- 'spanish_cds_ll_f1_f2.csv'
analyze_results(cds_file, cds_mu_file, cds_cov_file, cds_ll_file)
# Look at English ADS
ads_file <- 'english_ads_f1_f2.csv'
ads_mu_file <- 'english_ads_mus_f1_f2.csv'
ads_cov_file <- 'english_ads_covs_f1_f2.csv'
ads_ll_file <- 'english_ads_ll_f1_f2.csv'
analyze_results(ads_file, ads_mu_file, ads_cov_file, ads_ll_file)
# Look at English CDS
cds_file <- 'english_cds_f1_f2.csv'
cds_mu_file <- 'english_cds_mus_f1_f2.csv'
cds_cov_file <- 'english_cds_covs_f1_f2.csv'
cds_ll_file <- 'english_cds_ll_f1_f2.csv'
analyze_results(cds_file, cds_mu_file, cds_cov_file, cds_ll_file)
